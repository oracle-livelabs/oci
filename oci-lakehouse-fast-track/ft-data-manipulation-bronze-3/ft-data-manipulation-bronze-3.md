# Manipula√ß√£o de Dados - Bronze

## Introdu√ß√£o

Neste laborat√≥rio, voc√™ ir√° aprender a manipular um conjunto de dados seguindo o modelo da estrutura medalh√£o (bronze, prata e ouro) atrav√©s do Data Flow Studio, que foi configurado anteriormente dentro do OCI Data Science.

Ao final de cada etapa desta arquitetura medalh√£o, ser√° importante armazenar as tabelas modificadas em buckets. Isso √© necess√°rio devido √† natureza do processamento em mem√≥ria do Data Flow, onde os dados s√£o tempor√°rios e se perdem ap√≥s o encerramento da sess√£o, tornando essencial a persist√™ncia dos dados processados em um armazenamento dur√°vel como os buckets.

### *Compreendendo o Processamento e Armazenamento de Dados no Data Flow*

---

O funcionamento do Data Flow pode ser entendido de forma intuitiva como um processo que ocorre em uma "mem√≥ria" tempor√°ria. Aqui est√° uma explica√ß√£o simplificada:

**Processamento Tempor√°rio:**
   - Quando voc√™ inicia um Data Flow, ele cria um cluster tempor√°rio na nuvem.
   - Neste cluster, os dados s√£o carregados, processados e manipulados conforme necess√°rio. 
   - Pense nisso como um espa√ßo de trabalho tempor√°rio onde seus dados s√£o trazidos para an√°lise e transforma√ß√£o.

**Natureza Ef√™mera dos Dados no Cluster:**
   - Importante ressaltar que esses dados existem apenas enquanto o cluster est√° ativo.
   - Assim que voc√™ termina sua sess√£o e o cluster √© fechado, todos os dados processados nesse ambiente tempor√°rio s√£o perdidos.
   - Isso significa que qualquer processamento ou resultado obtido durante a sess√£o precisa ser armazenado de forma permanente em outro lugar, caso contr√°rio, ser√° perdido.

**Persist√™ncia dos Dados em Buckets:**
   - Para preservar os resultados do seu trabalho no Data Flow, voc√™ precisa salvar os dados processados em um local de armazenamento permanente.
   - No contexto do OCI Data Science, isso geralmente √© feito salvando os dados em "buckets", que s√£o essencialmente espa√ßos de armazenamento na nuvem.
   - Ao salvar os dados em um bucket, voc√™ garante que eles estar√£o dispon√≠veis para uso futuro, mesmo ap√≥s o encerramento da sess√£o do Data Flow.

> Em resumo, o Data Flow permite processar dados de forma poderosa e flex√≠vel em um ambiente tempor√°rio. No entanto, para reter os resultados desse processamento, √© crucial salvar os dados em um local permanente, como um bucket, antes de encerrar a sess√£o do cluster. Isso garante que voc√™ possa acessar e utilizar esses dados processados para an√°lises futuras ou para outras necessidades do projeto.


*Tempo estimado para o Lab:* 10 Minutos

### Objetivos


* Compreender a origem do conjunto de dados e an√°lises que ser√£o realizadas.

* Utilizar comandos interativos para executar Spark em notebooks.

* Configura√ß√£o da autentica√ß√£o e das vari√°veis de ambiente para acessar recursos na OCI.

* Carregamento de dados diretamente de um bucket da OCI para an√°lise.

* Armazenamento inicial de dados processados no formato otimizado Delta.

## Vis√£o Geral do Conjunto de Dados

O conjunto de dados apresentado √© origin√°rio do Governo Federal Brasileiro e fornece informa√ß√µes detalhadas usadas na constru√ß√£o da balan√ßa comercial do Brasil. 

O conjunto inclui dados do sistema *Comex Stat* e est√° organizado de acordo com a *Nomenclatura Comum do Mercosul (NCM)* ou por munic√≠pios de exportadores/importadores. Dispon√≠veis em formato CSV e separados por ponto e v√≠rgula (;), os dados que ser√£o analisados abrangem o per√≠odo de *2023* e incluem informa√ß√µes como ano, m√™s, c√≥digo NCM, unidade estat√≠stica, pa√≠s de origem/destino, UF de origem/destino, rota de transporte, √≥rg√£o respons√°vel pelo desembara√ßo (URF), quantidade estat√≠stica, peso l√≠quido em quilogramas e valor em d√≥lares FOB (Free on Board). 

O dicion√°rio de dados est√° disponibilizado abaixo.

| Campo                            | Descri√ß√£o |
|----------------------------------|-----------|
| **CO_ANO**                       | Ano (1997 a 2021) |
| **CO_MES**                       | C√≥digos de m√™s (1: janeiro, 12: dezembro) |
| **CO_NCM**                       | C√≥digo da Nomenclatura Comum do Mercosul - Utilizado para controlar e identificar mercadorias comercializadas no Brasil e nos demais pa√≠ses do Mercosul (cada NCM representa um produto diferente) |
| **Pa√≠ses do Mercosul**           | O Mercado Comum do Sul, conhecido pelas abreviaturas Mercosul (portugu√™s) e Mercosur (espanhol), √© um bloco comercial sul-americano estabelecido pelo Tratado de Assun√ß√£o em 1991 e pelo Protocolo de Ouro Preto em 1994, com membros plenos Argentina, Brasil, Paraguai e Uruguai. Fonte: Wikip√©dia |
| **CO_UNID**                      | C√≥digo da Unidade de Medida Estat√≠stica, uma unidade de medida padr√£o para cada NCM, incluindo valores como quilograma, metro, litro, pares, tonelada, entre outros. |
| **CO_PAIS**                      | C√≥digo do nome do pa√≠s com o qual foi realizada a opera√ß√£o comercial (importa√ß√£o ou exporta√ß√£o) |
| **SG\_UF\_NCM**                    | C√≥digo da Unidade Federativa (estado) de origem (exporta√ß√£o) ou destino (importa√ß√£o) da mercadoria. |
| **CO_VIA**                       | C√≥digo para identificar o meio de transporte utilizado (a√©reo, mar√≠timo, rodovi√°rio, ferrovi√°rio, etc.). Na exporta√ß√£o, refere-se ao meio de transporte da mercadoria para o exterior; na importa√ß√£o, ao meio de acesso da mercadoria ao territ√≥rio nacional. |
| **CO_URF**                       | C√≥digo do √≥rg√£o respons√°vel pelos tr√¢mites de desembara√ßo aduaneiro de mercadorias importadas/exportadas |
| **QT_ESTAT**                     | Unidade estat√≠stica de cada produto conforme discriminado por NCM, variando entre quilogramas l√≠quidos, n√∫mero de unidades, pares, dezenas, mil e toneladas. A tabela ‚ÄúNCM_UNIDADE‚Äù relaciona cada NCM √† sua unidade estat√≠stica correspondente. √â importante n√£o somar quantidades estat√≠sticas de NCMs com unidades diferentes. |
| **KG_NET**                       | Peso l√≠quido da mercadoria em quilogramas, desconsiderando embalagens ou transportes adicionais, dispon√≠vel mesmo para produtos com unidades estat√≠sticas diferentes de quilogramas. As informa√ß√µes fornecidas nas opera√ß√µes de com√©rcio exterior s√£o de responsabilidade dos operadores. |
| **VL_FOB**                       | Valor da mercadoria em d√≥lares americanos (USD $) no Incoterm FOB (Free on Board), onde o vendedor √© respons√°vel pelo envio da mercadoria e o comprador pelos custos de frete, seguro e outros custos p√≥s-embarque. |

### *An√°lise de Dados*

---

Durante o laborat√≥rio, buscaremos analisar as seguintes quest√µes relacionadas √†s rela√ß√µes comerciais entre o Brasil e os pa√≠ses do Mercosul:

- *Principais produtos (NCM) exportados e importados no Brasil para os pa√≠ses do Mercosul:* Identificaremos quais foram os produtos mais exportados ou importados pelo Brasil para os membros do Mercosul, utilizando o c√≥digo NCM como refer√™ncia para classificar e agrupar os produtos.

- *Estado brasileiro que mais exportou ou importou mercadorias nos √∫ltimos 6 meses:* Analisaremos os dados para determinar qual estado brasileiro foi o maior importador ou exportador de mercadorias nos 6 meses mais recentes dispon√≠veis nos dados.


## Tarefa 1: Comando Spark Magic

Para interagirmos com nosso cluster Spark do OCI Data Flow devemos sempre utilizar o magic command *`%%spark`* na c√©lula antes do comando que queremos executar no cluster.

Quando voc√™ insere o magic command *`%%spark`* em uma c√©lula do notebook e executa a c√©lula, o c√≥digo dentro dela n√£o √© executado localmente no seu ambiente Jupyter. Em vez disso, o comando indica que o c√≥digo deve ser enviado para processamento em um ambiente remoto, que no caso √© o Data Flow Session.

1. Se voc√™ executar o comando como indicado na imagem abaixo, ele ser√° executado localmente no Jupyter Notebook.

   ![Spark Comandos](.\images\1-print-local.png)

2. Agora se voc√™ executar o comando como indicado nesta outra imagem abaixo, ele ser√° executado no Cluster Spark do OCI Data Flow.

   ![Spark Comandos](.\images\2-print-flow.png)


Portanto, o comando *`%%spark`* funciona como uma ponte entre o seu notebook Jupyter e o cluster Spark remoto do Data Flow. Ele permite que voc√™ escreva c√≥digo Spark no notebook e tenha esse c√≥digo executado no cluster Spark.  Uma vez que o c√≥digo √© processado no cluster Spark, os resultados s√£o ent√£o enviados de volta e exibidos no seu ambiente OCI Data Science.



## Tarefa 2: Autentica√ß√£o e Cria√ß√£o de Vari√°veis do Ambiente

### *Autentica√ß√£o Spark*

---

Conforme o c√≥digo no laborat√≥rio anterior, a autentica√ß√£o √© realizada para obter automaticamente as vari√°veis que ser√£o utilizadas durante o processo. Isso √© feito utilizando o Resource Principal, um m√©todo de autentica√ß√£o que permite que os servi√ßos da Oracle Cloud Infrastructure (OCI) autentiquem chamadas a outros servi√ßos OCI sem a necessidade de gerenciar credenciais expl√≠citas.

Al√©m de preparar o ambiente para trabalhar com Apache Spark na Oracle Cloud Infrastructure (OCI) e configura a autentica√ß√£o necess√°ria para acessar outros servi√ßos da OCI, como buckets do Object Storage. Ele tamb√©m garante que vers√µes mais recentes e seguras de bibliotecas de criptografia sejam utilizadas.

1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

   ![Autentica√ß√£o Spark](.\images\2-authentication-spark.png)

### *Cria√ß√£o de Vari√°veis do Ambiente*

---

O c√≥digo abaixo tem como objetivo buscar o ID do compartimento na Oracle Cloud Infrastructure (OCI). No entanto, ao contr√°rio do que foi feito no laborat√≥rio 2, onde uma vari√°vel de ambiente dispon√≠vel no ambiente do Data Science da OCI foi utilizada para obter essa informa√ß√£o, neste contexto atual n√£o temos acesso a essa vari√°vel de ambiente para realizar a mesma opera√ß√£o. Portanto, o c√≥digo precisa de um m√©todo alternativo para obter o ID do compartimento, que neste caso √© feito atrav√©s da listagem de todos os compartimentos acess√≠veis e buscando Data Flows em cada um para encontrar o ID desejado.

2. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

   ![Cria√ß√£o Vari√°veis Identificadores](.\images\3-compartment-id.png)

> **Porque devo declarar novamente as vari√°veis?** Como mencionado anteriormente, existe uma distin√ß√£o importante entre as c√©lulas executadas com `%%spark` e aquelas sem ele no nosso ambiente. As c√©lulas com `%%spark` s√£o processadas dentro do Data Flow Studio, ao passo que as c√©lulas sem ele s√£o executadas no OCI Data Science. Essa √© a raz√£o pela qual precisamos declarar a informa√ß√£o do namespace do bucket mais uma vez ‚Äì pois nosso contexto mudou do Data Science para o Data Flow Studio, onde a informa√ß√£o ser√° utilizada novamente.

Em seguida, executaremos o c√≥digo abaixo, que apresenta como objetivo buscar o namespace do Object Storage da OCI e identificar os nomes espec√≠ficos dos buckets categorizados como 'bronze', 'prata', 'ouro' e 'metastore'. Ele autentica usando Resource Principals, obt√©m o namespace, lista os buckets no compartimento designado e filtra essa lista para obter os nomes dos buckets de acordo com os crit√©rios definidos.

Os buckets obtidos ser√£o utilizados para armazenamento dos dados na arquitetura medalh√£o em cada etapa durante o processo.

3. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

   ![Spark Vari√°veis](.\images\4-spark-variables.png)

## Tarefa 3: Importando Dados do Bucket 

Neste passo, estamos definindo a vari√°vel *`csv_file_path`* para armazenar o caminho do arquivo CSV e a vari√°vel *`json_file_path_paises` para o caminho do arquivo JSON que queremos importar. 

Estamos buscando os arquivo em um bucket BRONZE, chamado *`oci://+bucket_bronze+@+namespace+`*, cujas vari√°veis foram definidas acima. Em seguida, estamos utilizando o Spark para ler o arquivo CSV e em seguida, JSON. 

1. Selecione A c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

   ![Importanto Bucket CSV](.\images\5-import-csv.png)

O primeiro arquivo, identificado como *`EXPORTACAO_BRASIL_LIVELABS`* ser√° nosso dataset principal, com as informa√ß√µes listadas em "Vis√£o Geral do Conjunto de Dados", primeira etapa deste laborat√≥rio. O segundo arquivo *`CODIGO_PAISES_LIVELABS.json`* cont√©m informa√ß√µes sobre c√≥digos de pa√≠ses, incluindo os nomes dos pa√≠ses codificados numericamente em nosso dataset original. O terceiro e quarto arquivos, *`CODIGO_NCM.xlsx`* e *`CODIGO_VIA.csv`*, cont√©m c√≥digos NCM, que s√£o usados para classificar mercadorias em categorias para fins de com√©rcio e tarifa√ß√£o, e o nome das vias de exporta√ß√£o (mar√≠tima, fluvial, etc.), informa√ß√£o esta que est√° codificada no dataset original.

> O comando *`printSchema()`* √© usado para detalhar as colunas, tipos de dados e se as colunas aceitam valores nulos em cada DataFrame. 

2. Selecione cada c√©lula e execute-as com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

   ![Importanto Bucket JSON C√≥digo Paises](.\images\6-country-code.png)

   ![Importanto Bucket CSV VIA](.\images\7-import-csv-via.png)

   ![Importanto Bucket CSV NCM](.\images\8-ncm-code.png)

O primeiro arquivo, identificado como *`CODIGO_PAISES_LIVELABS.json`*, cont√©m informa√ß√µes sobre c√≥digos de pa√≠ses, incluindo os nomes dos pa√≠ses codificados numericamente em nosso dataframe original. O segundo arquivo, *`CODIGO_NCM.xlsx`*, cont√©m c√≥digos NCM, que s√£o usados para classificar mercadorias em categorias para fins de com√©rcio e tarifa√ß√£o.

Na camada ouro, planejamos unir esses conjuntos de dados ao nosso dataset principal. Essa uni√£o nos permitir√° realizar an√°lises mais ricas, correlacionando os c√≥digos num√©ricos dos pa√≠ses e os c√≥digos de classifica√ß√£o de mercadorias (NCM) com as informa√ß√µes correspondentes no dataset principal. 

### *A busca de arquivos no Object Storage (Bucket)*

---

Na estrutura *`oci://"+bucket_bronze+"@"+namespace+"/EXPORTACAO_BRASIL_LIVELABS.csv`*, podemos identificar os seguintes componentes:

**Nome do Bucket**: Conte√∫do da vari√°vel "+bucket_bronze+"

Isso representa o nome do bucket de armazenamento.

**Namespace**: Conte√∫do da vari√°vel "+namespace+"

O namespace √© um identificador √∫nico do OCI Object Storage, cada conta na Oracle Cloud recebe o seu.

**Nome do Arquivo**: BRASIL\_EXPORTACAO\_LIVELABS.csv

Este √© o nome do arquivo espec√≠fico localizado dentro do bucket bronze. No contexto desse c√≥digo, √© o arquivo CSV que est√° sendo lido e processado pelo Spark.


## Tarefa 4: Salvando um DataFrame como um Arquivo Delta

Neste c√≥digo, estamos utilizando o Apache Spark para salvar um DataFrame derivado de um arquivo CSV e outros de arquivos JSON no formato *Delta Lake* em um bucket, usando caminhos espec√≠ficos para cada tipo de dado.

   1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

   ![Save Delta Bronze](.\images\9-save-delta-bronze.png)

### *O que √© um Arquivo Delta?*

---

Pense num arquivo Delta Lake como um √°lbum de fotos digital onde voc√™ pode facilmente adicionar, editar ou deletar fotos. Se voc√™ cometer um erro, √© f√°cil desfazer a a√ß√£o e recuperar a foto como estava antes. Assim como um √°lbum mant√©m um registro das suas fotos, o Delta Lake mant√©m um registro das altera√ß√µes nos seus dados, tornando o acesso e a gest√£o deles simples e seguros.

Desta forma, corresponde a um tipo de formato de armazenamento que oferece v√°rias vantagens para o processamento e an√°lise de grandes volumes de dados. Ele √© constru√≠do em cima do Apache Spark e otimiza opera√ß√µes de leitura e grava√ß√£o, mantendo a integridade dos dados atrav√©s de transa√ß√µes ACID, que s√£o mecanismos usados para garantir que todas as opera√ß√µes de dados sejam realizadas com seguran√ßa e sem erros.

Salvar dados neste formato √© √∫til porque permite:

- *Gerenciamento de Metadados:* Facilita o rastreamento das altera√ß√µes feitas nos dados.
- *Escala:* Lida eficientemente com grandes quantidades de dados.
- *Desempenho:* Melhora a velocidade de leitura e escrita dos dados.
- *Confiabilidade:* Garante que os dados n√£o sejam corrompidos, mesmo com v√°rias opera√ß√µes simult√¢neas.

Parab√©ns, voc√™ terminou esse laborat√≥rio! üéâ

Voc√™ pode **seguir para o pr√≥ximo Lab**.

## Conclus√£o

Nesta sess√£o, voc√™ aprendeu a manipular dados nas camadas bronze, prata e ouro de um data lake na Oracle Cloud Infrastructure (OCI), come√ßando pela importa√ß√£o de conjuntos de dados brutos do Object Storage, passando pela autentica√ß√£o segura com Resource Principals, at√© o refinamento e armazenamento de dados em formatos otimizados para an√°lises aprofundadas.

## Autoria

- **Autores** - Thais Henrique, Heloisa Escobar, Isabelle Anjos
- **√öltimo Update Por/Date** - Isabelle Anjos, Jan/2024