# Manipula√ß√£o de Dados - Prata

## Introdu√ß√£o

Neste laborat√≥rio, estaremos trabalhando com a camada prata do nosso data lake, que √© crucial para refinar e transformar os dados brutos importados da camada bronze em um formato mais anal√≠tico e pronto para insights. A camada prata serve como um est√°gio interm√©dio onde os dados s√£o limpos, deduplicados, padronizados e enriquecidos.

Os principais tratamentos de dados que realizaremos incluem:

- *Leitura dos Arquivos Parquet:* Carregar dados armazenados no formato eficiente Parquet, que j√° cont√™m a estrutura e tipos de dados adequados para processamento.

- *Achatamento de JSON:* Converter estruturas JSON complexas e aninhadas em formatos tabulares para facilitar an√°lises e opera√ß√µes de dados.

- *Remo√ß√£o de Duplicatas:* Identificar e eliminar registros duplicados para garantir a integridade e a confiabilidade dos dados.

- *Padroniza√ß√£o de Colunas:* Renomear colunas para remover prefixos desnecess√°rios e padronizar nomes para consist√™ncia em todo o conjunto de dados.

- *Tratamento de Valores Nulos:* Preencher ou substituir valores nulos com valores padr√£o para manter a integridade dos dados durante an√°lises subsequentes.

- *Convers√£o de Tipos de Dados:* Converter tipos de dados de colunas espec√≠ficas para formatos apropriados, exceto para colunas que requerem manuten√ß√£o de formatos num√©ricos para an√°lises quantitativas.

Cada uma dessas etapas √© fundamental para assegurar que os dados estejam prontos para uso em an√°lises mais profundas e para a tomada de decis√µes baseada em dados na camada ouro, onde o foco est√° em insights e relat√≥rios


*Tempo estimado para o Lab:* 10 Minutos

### Objetivos

* Processar dados Parquet da camada bronze.
* Aplanar JSON para tabelas.
* Remover duplicatas.
* Padronizar nomes de colunas.
* Tratar valores nulos.
* Converter tipos de dados adequados.
* Salvar como arquivo Delta.

## Tarefa 1: Leitura de arquivo Parquet

Agora vamos avan√ßar para o processamento e an√°lise dos dados que importamos anteriormente, assegurando que eles estejam prontos para uso futuro.

### *Leitura dos Arquivos Parquet no Bucket*

Na etapa anterior, armazenamos nossos DataFrames no formato Parquet. Este script est√° acessando esses dados de dois locais distintos, correspondendo aos conjuntos de dados originados de arquivos CSV e JSON.

 A op√ß√£o *`header=True`* indica que a primeira linha dos arquivos Parquet cont√©m os cabe√ßalhos das colunas, e *`inferSchema=True`* permite que o Spark infira automaticamente o esquema dos dados (tipos de colunas) com base nos dados.

1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

    ![Leitura Parquet Bronze](.\images\1-read-parquet.png)

## Tarefa 2: Achatamento JSON e Padroniza√ß√£o de Colunas

### *Fun√ß√£o do Achatamento JSON*

---

Este c√≥digo em Spark cont√©m duas fun√ß√µes definida para lidar com DataFrames que t√™m colunas estruturadas de forma complexa, como arrays aninhados e estruturas (structs).

1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

    ![Flatten Function](.\images\2-function-flatten.png)

Em resumo, estas fun√ß√µes est√£o transformando um DataFrame com colunas complexas e aninhadas em um formato mais simples e tabular, onde cada dado aninhado √© trazido para uma coluna pr√≥pria de n√≠vel superior, facilitando a an√°lise e o processamento dos dados.

2. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook.

    ![Execu√ß√£o Flatten Function](.\images\3-execute-flatten.png)

### **Porque achatamos o arquivo JSON?**

---

Converter dados aninhados JSON para um formato tabular √© √∫til porque a maioria das ferramentas de an√°lise de dados e bancos de dados trabalham melhor com dados em formato de tabela, onde cada coluna cont√©m valores de um √∫nico tipo de dado e cada linha representa um registro √∫nico. JSONs aninhados podem ser complexos e dif√≠ceis de analisar, pois as estruturas de dados aninhadas n√£o se alinham bem com as estruturas de tabela.

Por exemplo, considere uma parte do JSON que iremos utilizar antes do achatamento:

![Exemplo de arquivo json](.\images\json-print.png)


Este JSON cont√©m dados aninhados sob a chave "data". Ap√≥s achat√°-lo, voc√™ ter√° um formato tabular onde cada campo aninhado se torna uma coluna separada:

| data\_CO\_PAIS | data\_CO\_PAIS\_ISON3 | data\_CO\_PAIS\_ISOA3 | data\_NO\_PAIS       | data\_NO\_PAIS\_ING  | data\_NO\_PAIS\_ESP  |
|---------|---------------|---------------|---------------|--------------|--------------|
| 0       | 898           | ZZZ           | N√£o Definido  | Not defined  | No definido  |
| 13      | 4             | AFG           | Afeganist√£o   | Afghanistan  | Afganistan   |
| 15      | 248           | ALA           | Aland         | (empty)      | (empty)      |

### *Padroniza√ß√£o de Colunas*

A padroniza√ß√£o das colunas em um conjunto de dados √© uma pr√°tica importante que facilita a manipula√ß√£o, a an√°lise e a integra√ß√£o dos dados, especialmente quando se est√° trabalhando com v√°rias fontes de dados ou grandes conjuntos de dados.

Neste c√≥digo, estamos realizando a renomea√ß√£o de colunas do DataFrame. Cada coluna que tem o prefixo "data\_" no seu nome ter√° esse prefixo removido. O c√≥digo percorre todas as colunas do DataFrame e, para cada uma, cria um novo nome sem o prefixo "data\_".

![Padroniza√ß√£o de Colunas](.\images\6-padronized-columns.png)

## Tarefa 3: Retirando Duplicatas 

### *Tratamento de Duplicatas*

Primeiro, contamos o n√∫mero de linhas e colunas para entender o tamanho original do nosso DataFrame chamado *¬¥df¬¥*. Para isso, usamos *¬¥df.count()¬¥* para contar as linhas e *¬¥len(df.columns)¬¥* para contar as colunas. Em seguida, imprimimos esses n√∫meros para gerar um registro.

![N√∫mero de linhas e colunas](.\images\4-number-lines-original.png)

Depois disso, removemos as linhas duplicadas com o m√©todo *¬¥dropDuplicates()¬¥*. Por fim, verificamos novamente a contagem de linhas para identificar quantas foram removidas.

![Tratamento Duplicatas](.\images\5-drop-duplicates.png)

## Tarefa 4: Preenchimento de Valores Nulos

### *Preenchimento de Valores Nulos*

Nesta etapa, estamos verificando individualmente cada coluna para verificar se h√° valores nulos usando *¬¥df.filter(col(col_name).isNull()).count()¬¥*. Se o resultado for maior que zero, isso significa que h√° valores nulos na coluna e a coluna √© adicionada √† lista null\_columns, caso existam, imprimimos o n√∫mero de valores nulos em cada coluna. Caso n√£o existam, a mensagem *"N√£o h√° valores nulos em nenhuma coluna do DataFrame."* ser√° exibida.

![Valores Nulos](.\images\7-null-values.png)

Neste trecho de c√≥digo, est√° ocorrendo um processo de tratamento de valores nulos (null) em nosso DataFrame chamado df. Para cada coluna, o c√≥digo verifica se o tipo de dado √© booleano (BooleanType). Se for, todos os valores nulos naquela coluna s√£o substitu√≠dos por False. Se o tipo de dado n√£o for booleano, os valores nulos s√£o substitu√≠dos por 0.

![Retirar Valores Nulos](.\images\8-replace-null-values.png)

Para verificar a presen√ßa de valores nulos em todas as colunas ap√≥s a aplica√ß√£o do c√≥digo que substitui os nulos por valores padr√£o, voc√™ pode realizar uma contagem de nulos por coluna. No Apache Spark, voc√™ pode fazer isso usando a fun√ß√£o *¬¥agg()¬¥* juntamente com a fun√ß√£o *¬¥count()¬¥* para contar explicitamente os nulos em cada coluna.

Ap√≥s a execu√ß√£o deste c√≥digo, voc√™ deve esperar ver zeros em todas as colunas, indicando que n√£o h√° mais valores nulos no DataFrame, gra√ßas ao preenchimento de nulos que foi realizado anteriormente no c√≥digo.

![Verificar Valores Nulos](.\images\9-verify-null-values.png)

## Tarefa 5: Convers√£o do Tipo de Dado e Salvar o Dataframe como Delta

### *Convers√£o de Dados*

Neste c√≥digo, estamos realizando a convers√£o do tipo de dados de v√°rias colunas para string (texto), com exce√ß√£o de duas colunas espec√≠ficas.
As colunas KG\_LIQUIDO e VL\_FOB n√£o foram convertidas para o tipo string porque elas cont√™m informa√ß√µes num√©ricas essenciais que ser√£o utilizadas em c√°lculos e an√°lises estat√≠sticas.

![Convers√£o de Dados](.\images\10-convert-type-variable.png)

### *Salvando o Dataframe como Arquivo Delta*

Ap√≥s realizar todos os tratamentos necess√°rios no nosso DataFrame, estamos prontos para salv√°-lo em um formato otimizado Delta Lake em nosso bucket prata. O arquivo Delta que estamos salvando ser√° a base para as pr√≥ximas etapas de explora√ß√£o de dados e para responder a quest√µes importantes do neg√≥cio.

![Dataframe Delta Prata](.\images\11-delta-silver.png)

Parab√©ns, voc√™ terminou esse laborat√≥rio! üéâ

Voc√™ pode **seguir para o pr√≥ximo Lab**.

## Conclus√£o

Nesta sess√£o voc√™ aprendeu a aprimorar a qualidade dos dados na camada prata do data lake, garantindo que eles estejam limpos, padronizados e prontos para an√°lises mais detalhadas. Esses passos s√£o fundamentais para construir um pipeline de dados robusto e confi√°vel, permitindo que a tomada de decis√µes seja feita com base em informa√ß√µes precisas e confi√°veis. 

## Autoria

- **Autores** - Thais Henrique, Heloisa Escobar, Isabelle Anjos
- **√öltimo Update Por/Date** - Isabelle Anjos, Jan/2024