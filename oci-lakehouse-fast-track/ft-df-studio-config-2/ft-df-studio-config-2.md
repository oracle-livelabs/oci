# Configura√ß√£o do Data Flow Studio no OCI Data Science

## Introdu√ß√£o

Neste laborat√≥rio, voc√™ ir√° aprender como configurar uma sess√£o no Data Flow Studio, um componente integrado ao OCI Data Science. 

O ambiente que voc√™ estar√° configurando e utilizando √© a OCI Data Flow Session. Esta √© a plataforma onde voc√™ processar√° os dados, aproveitando os recursos interativos para desenvolver, testar e otimizar fluxos de dados em tempo real.

Ao final deste laborat√≥rio, voc√™ ter√° desenvolvido habilidades essenciais para efetivamente criar e gerenciar fluxos de dados, permitindo-lhe trabalhar com mais efici√™ncia em projetos de an√°lise de dados e ci√™ncia de dados.


### *O que √© o Data Flow?*

---

O OCI Data Flow √© um servi√ßo totalmente gerenciado na Oracle Cloud Infrastructure (OCI) destinado a executar scripts Apache Spark sem a necessidade de gerenciar infraestrutura ou instalar software. Este servi√ßo √© especialmente vantajoso para empresas e desenvolvedores que necessitam processar grandes volumes de dados de forma eficiente. Com o OCI Data Flow, os usu√°rios podem criar aplica√ß√µes em Java, Scala, Python ou SQL, que s√£o posteriormente executadas em ambientes Apache Spark. Ele √© ideal para tarefas como processamento de dados em lote, opera√ß√µes de ETL (Extract, Transform, Load), e an√°lise de dados complexos. A principal vantagem do OCI Data Flow √© sua escalabilidade e facilidade de uso, permitindo aos usu√°rios concentrarem-se na escrita de seus scripts sem se preocuparem com o gerenciamento da infraestrutura subjacente.

### *O que √© o Data Flow Studio?*

---

Diferentemente do OCI Data Flow, o Data Flow Studio √© uma extens√£o ou uma camada adicional que oferece um ambiente a partir de uma **ferramenta integrada ao OCI Data Science**, com um conjunto de ferramentas que permite aos usu√°rios aproveitar as funcionalidades de processamento de dados em conjunto com outras ferramentas de ci√™ncia de dados. Al√©m disso, o Data Flow Studio √© ideal para realizar opera√ß√µes de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) atrav√©s de c√≥digo.

A capacidade de realizar ETL por meio de c√≥digo proporciona uma grande flexibilidade, permitindo a cria√ß√£o de processos personalizados para extrair dados de diversas fontes, transform√°-los de acordo com as necessidades espec√≠ficas e, em seguida, carreg√°-los para destinos apropriados. 

| OCI Data Flow                                    | OCI Data Flow Studio                              |
| ------------------------------------------------- | ------------------------------------------------- |
| - Servi√ßo totalmente gerenciado para executar scripts Apache Spark. | - Extens√£o ou camada adicional ao OCI Data Flow. |
| - N√£o requer infraestrutura para gerenciar nem instalar software. | - Oferece um ambiente de desenvolvimento integrado (IDE) na nuvem. |
| - Escal√°vel, permite o processamento de grandes volumes de dados. | - Permite construir, testar e executar aplica√ß√µes Spark. |
| - Suporta aplica√ß√µes em Java, Scala, Python ou SQL no Apache Spark. | - Interface gr√°fica para intera√ß√£o intuitiva com dados e scripts. |
| - Ideal para processamento de dados em lote, ETL, e an√°lise de dados. | - Adequado para uma abordagem mais visual e interativa no desenvolvimento de pipelines de dados. |
|                                                   | - Ferramenta integrada ao OCI Data Science.       |



### *Porque utilizar o Data Flow Studio?*

--- 
O Data Flow Studio √© uma ferramenta altamente especializada para tarefas que envolvem **processamento de dados em grande escala**, fornecendo um ambiente otimizado com recursos de escalabilidade autom√°ticos. Isso permite lidar com picos de demanda sem a necessidade de gerenciar a infraestrutura manualmente. Sua interface intuitiva e ferramentas especializadas s√£o ideais para manipular grandes volumes de dados, tornando o processo mais simples e eficiente. 
 
Al√©m disso, o Data Flow Studio proporciona um ambiente integrado e interativo, essencial para o desenvolvimento e teste de fluxos de dados em tempo real. Essa caracter√≠stica √© crucial para permitir a r√°pida itera√ß√£o e ajuste dos processos de dados.

> *Desta forma, corresponde a escolha ideal se o foco principal do seu projeto est√° no processamento e manipula√ß√£o eficientes de grandes volumes de dados. Ele oferece um ambiente espec√≠fico e otimizado para essa finalidade.*

### *Observa√ß√£o:*

---

Quando voc√™ iniciar o notebook *`LiveLabs.ipynb`*, encontrar√° no sum√°rio uma rela√ß√£o completa dos assuntos que ser√£o explorados durante os laborat√≥rios 2 e 3.

> Para executar o c√≥digo de qualquer c√©lula no notebook, primeiro selecione a c√©lula que deseja executar. Em seguida, voc√™ pode pressionar *`SHIFT + ENTER`* no teclado ou clicar no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook para realizar a a√ß√£o.  ![Execu√ß√£o Notebook](.\images\0-execution.png)

*Tempo estimado para o Lab:* 10 Minutos

### Objetivos

* Compreender a estrutura e funcionalidades de um notebook Jupyter.
* Aprender os atalhos de teclado e comandos espec√≠ficos do ambiente OCI Data Science.
* Dominar o processo de autentica√ß√£o usando o Oracle Accelerated Data Science SDK.
* Entender como configurar e utilizar vari√°veis de ambiente importantes no notebook.
* Aprender a usar comandos m√°gicos para intera√ß√£o com o Apache Spark.
* Desenvolver compet√™ncias para criar, monitorar e verificar o status de sess√µes do Spark.

## Aprendendo sobre Notebook Jupyter e comandos OCI Data Science

### *Como funciona um Notebook Jupyter:*

---

No √∫ltimo laborat√≥rio, fizemos a importa√ß√£o de um notebook Jupyter para utiliz√°-lo no ambiente Data Science. Mas, voc√™ pode se perguntar, **o que exatamente √© um notebook Jupyter e como ele funciona?**

Um notebook Jupyter √© uma ferramenta interativa muito popular entre cientistas de dados, engenheiros de dados e analistas. Ele combina a execu√ß√£o de c√≥digo com a adi√ß√£o de notas, gr√°ficos e outros elementos visuais, tudo em um √∫nico documento. Vejamos como funciona:

### **C√©lulas:**

O cora√ß√£o de um notebook Jupyter s√£o as "c√©lulas", que s√£o blocos onde voc√™ pode escrever c√≥digo ou texto. Desta forma, existem dois tipos principais de c√©lulas:


- *C√©lulas de C√≥digo:* Onde voc√™ escreve e executa c√≥digos em linguagens como Python, R, Pyspark, etc. Quando voc√™ executa uma c√©lula de c√≥digo (geralmente usando o comando Shift + Enter), o notebook processa o c√≥digo e exibe o resultado logo abaixo da c√©lula.

- *C√©lulas de Markdown:* Usadas para adicionar texto explicativo, imagens, links e formata√ß√£o, como t√≠tulos e listas, para documentar o que o c√≥digo est√° fazendo. Markdown √© uma linguagem de marca√ß√£o leve que permite formata√ß√µes b√°sicas (como negrito, it√°lico) e avan√ßadas (como listas, tabelas e c√≥digo HTML).Essas c√©lulas n√£o s√£o "executadas" como as c√©lulas de c√≥digo, mas sim "renderizadas" para exibir o texto formatado.

### **Execu√ß√£o Interativa**
O que torna os notebooks Jupyter atraentes √© a capacidade de executar o c√≥digo de maneira interativa. Voc√™ pode modificar o c√≥digo, execut√°-lo e ver os resultados imediatamente. As c√©lulas podem ser executadas em qualquer ordem, o que permite testar pequenos trechos de c√≥digo rapidamente sem ter que executar todo o notebook.

### **Versatilidade**
Os notebooks s√£o ideais para uma variedade de tarefas, incluindo an√°lise de dados, visualiza√ß√£o, modelagem estat√≠stica e machine learning, bem como para criar tutoriais e guias educacionais.

### **Compartilhamento e Colabora√ß√£o**
Os notebooks podem ser facilmente compartilhados como arquivos (geralmente com a extens√£o .ipynb) e podem ser visualizados atrav√©s de plataformas como o GitHub ou executados em ambientes como o OCI Data Science.

<br>

### *Comandos OCI Data Science*

---

Segue uma rela√ß√£o de comandos dispon√≠veis no ambiente do OCI Data Science. Para usar esses atalhos, voc√™ precisa estar em modo de comando (pressionando Esc antes do atalho):

| Atalho de Teclado               | A√ß√£o                                       |
|---------------------------------|--------------------------------------------|
| `Shift + Enter`                 | Executar a c√©lula atual e ir para a pr√≥xima|
| `Ctrl + Enter`                  | Executar a c√©lula atual                    |
| `Alt + Enter`                   | Executar a c√©lula atual e inserir uma nova |
| `Esc` e depois `A`              | Inserir c√©lula acima                       |
| `Esc` e depois `B`              | Inserir c√©lula abaixo                      |
| `Esc` e depois `D`, `D`         | Excluir c√©lula atual                       |
| `Esc` e depois `M`              | Mudar c√©lula atual para tipo Markdown      |
| `Esc` e depois `Y`              | Mudar c√©lula atual para tipo C√≥digo        |
| `Esc` e depois `L`              | Alternar n√∫meros de linha                  |
| `Esc` e depois `C`              | Copiar c√©lula                              |
| `Esc` e depois `X`              | Cortar c√©lula                              |
| `Esc` e depois `V`              | Colar c√©lula abaixo                        |
| `Esc` e depois `I`, `I`         | Interromper a execu√ß√£o da c√©lula           |


## Tarefa 1: Autentica√ß√£o

Para garantir um processo de an√°lise de dados suave e eficiente no Data Flow Studio, √© essencial estabelecer corretamente os m√©todos de autentica√ß√£o. Nesta se√ß√£o focaremos no *T√≥pico 1*, que √© dedicado a este processo.

O Oracle Accelerated Data Science SDK (ADS) controla o mecanismo de autentica√ß√£o com o cluster Spark em uma Sess√£o do OCI Data Flow. Para configurar a autentica√ß√£o, utilizamos *`ads.set_auth("resource_principal")`*

O par√¢metro *`resource_principal`* indica que est√° sendo utilizado um m√©todo de autentica√ß√£o espec√≠fico, associado a recursos e pol√≠ticas de seguran√ßa dentro da infraestrutura da Oracle Cloud. Assim, o c√≥digo est√° preparando o ambiente para que o usu√°rio possa interagir de forma segura com o Spark, garantindo que todas as opera√ß√µes realizadas estejam autenticadas e autorizadas de acordo com as configura√ß√µes do OCI Data Science.

1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![Realizando Autentica√ß√£o](.\images\1-authentication.png)

O c√≥digo abaixo realiza a autentica√ß√£o para usar servi√ßos da Oracle Cloud Infrastructure, especialmente o Data Flow Studio. Ele importa bibliotecas necess√°rias, incluindo o SDK da OCI, e configura um assinante de princ√≠pios de recursos, que permite fazer solicita√ß√µes autenticadas aos servi√ßos da OCI sem gerenciar manualmente as credenciais.

2. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![Realizando Autentica√ß√£o](.\images\2-authentication-df.png)


## Tarefa 2: Cria√ß√£o de Vari√°veis

Este c√≥digo est√° configurando algumas vari√°veis importantes para serem utilizadas durante a *cria√ß√£o da sess√£o Spark*. 

1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![Vari√°veies Notebook](.\images\2-variables.png)

### *Informa√ß√£o sobre o compartment ID*

Primeiramente, ele importa a biblioteca *`os`*, que permite trabalhar com vari√°veis de ambiente do sistema operacional. Com isso, ele obt√©m o valor da vari√°vel de ambiente *`NB_SESSION_COMPARTMENT_OCID`*, que √© usada para identificar o compartimento espec√≠fico na Oracle Cloud Infrastructure (OCI) onde o notebook est√° armazenado. Esse valor √© ent√£o armazenado na vari√°vel *`compartment_id`*.

### *Informa√ß√£o sobre o bucket namespace*

Esse c√≥digo configura um cliente para acessar o servi√ßo de armazenamento de objetos da Oracle Cloud Infrastructure (OCI) e obt√©m o namespace do bucket de armazenamento. O cliente √© inicializado com configura√ß√µes padr√£o e autentica√ß√£o baseada no assinante configurado anteriormente. Em seguida, ele chama o m√©todo para obter o *namespace do bucket*, que √© um identificador √∫nico usado na OCI para organizar e separar recursos de armazenamento.

### *Informa√ß√£o sobre o nome dos buckets*

O c√≥digo lista todos os buckets em um namespace e compartimento espec√≠ficos da Oracle Cloud Infrastructure. Em seguida, ele procura e armazena os nomes dos primeiros buckets encontrados que cont√™m 'logs' e 'conda' em seus nomes, respectivamente, em duas vari√°veis diferentes.

### *Informa√ß√£o sobre o Metastore ID*

Esta etapa do c√≥digo configura um cliente para o Data Catalog da OCI, lista os metastores em um compartimento especificado e extrai o ID do primeiro metastore encontrado, armazenando-o em uma vari√°vel.

## Tarefa 3: Data Flow Spark Magic

### *Apache Spark e Data Flow*

---

O Apache Spark √© uma plataforma open source (c√≥digo aberto) para processamento de grandes volumes de dados. Uma das suas principais vantagens √© ser livremente acess√≠vel e modific√°vel por qualquer pessoa, o que fomenta uma comunidade ativa de desenvolvedores e usu√°rios que contribuem para sua evolu√ß√£o e aprimoramento cont√≠nuos.

No contexto do OCI Data Flow, que √© um servi√ßo oferecido pela Oracle Cloud Infrastructure, o Apache Spark desempenha um papel fundamental por tr√°s dos panos. O OCI Data Flow utiliza o Apache Spark como seu motor de processamento de dados. Isso significa que, quando voc√™ executa tarefas de an√°lise de dados, processamento de big data, ou qualquer opera√ß√£o relacionada a dados no OCI Data Flow, est√° utilizando a poderosa capacidade de processamento do Spark.

O Apache Spark √© especialmente conhecido por sua capacidade de processar rapidamente grandes conjuntos de dados, distribuindo tarefas de processamento em clusters de computadores. Isso o torna ideal para ambientes de nuvem como o OCI, onde escalabilidade e efici√™ncia s√£o cruciais. Ao ser integrado ao OCI Data Flow, o Spark permite que os usu√°rios executem tarefas complexas de processamento de dados, an√°lises estat√≠sticas e algoritmos de machine learning de maneira eficiente e escal√°vel, aproveitando a infraestrutura robusta da Oracle Cloud.

### *Helpers*

---

O c√≥digo apresentado define uma fun√ß√£o auxiliar chamada `prepare_command`. uma ferramenta de conveni√™ncia para transformar argumentos que s√£o armazenados em vari√°veis Python ou estruturas de dados em uma forma que pode ser utilizada pelos comandos m√°gicos do Spark, que muitas vezes esperam argumentos em formato de string.

1. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![C√≥digo Helpers](.\images\3-helpers.png)

Esses comandos m√°gicos permitem interagir de maneira eficiente com o Spark, facilitando a execu√ß√£o de opera√ß√µes complexas de processamento de dados de forma mais simplificada e integrada dentro do ambiente do notebook Jupyter. 

### *Data Flow Spark Magic*

---

O Data Flow Spark Magic refere-se a uma cole√ß√£o de "comandos m√°gicos" especializados no Jupyter Notebook que facilitam a intera√ß√£o com *Apache Spark*. Esses comandos s√£o poss√≠veis gra√ßas √† integra√ß√£o com a API REST do Apache Livy, um servi√ßo que permite a submiss√£o de trabalhos Spark de forma remota.

> O c√≥digo Spark escrito nos notebooks Jupyter √© executado em clusters Spark hospedados remotamente, permitindo lidar com grandes conjuntos de dados e tarefas de processamento intensivo.

Voc√™ precisa ativar o  Data Flow Spark Magic em seu notebook usando o comando m√°gico *`%load_ext dataflow.magics.`*

2. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![C√≥digo Data Flow Magic](.\images\4-load-spark-magic.png)

Ap√≥s a ativa√ß√£o da extens√£o, o comando *`%help`* pode ser usado para obter a lista de todos os comandos dispon√≠veis, juntamente com uma lista de seus argumentos e exemplos de chamadas.

3. Selecione a c√©lula e execute-a com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![C√≥digo Helpers](.\images\5-help.png)

> **[OPCIONAL] Doctring**: Para entender o prop√≥sito e os argumentos de um comando m√°gico no Jupyter Notebook, voc√™ pode adicionar um *`?`* ao final do comando, o que exibir√° a docstring, um texto explicativo incorporado no c√≥digo que fornece detalhes sobre sua funcionalidade. ![C√≥digo Docstring](.\images\1-callout-docstring.png)

## Tarefa 4: Criando uma Sess√£o

Neste momento do tutorial, vamos reutilizar algumas vari√°veis que foram criadas anteriormente na **Tarefa 3**. Vamos integr√°-las na cria√ß√£o da sess√£o do Data Flow Studio.

Para criar uma nova sess√£o de cluster do Data Flow, vamos utilizar o comando m√°gico *`%create_session`*.
De forma geral, este comando est√° configurando e iniciando uma nova sess√£o de cluster do Data Flow no OCI Data Science, especificando detalhes como o tipo de m√°quina virtual, configura√ß√µes de CPU e mem√≥ria, vers√£o do Spark, e onde armazenar os logs. 

1. Selecione cada uma das c√©lulas e execute-as com o comando SHIFT + ENTER, ou clique no bot√£o de execu√ß√£o (√≠cone de 'play') no notebook**.

    ![Criando sess√£o](.\images\6-create-session.png)

    ![Criando sess√£o comando](.\images\7-create-session-command.png)

Vamos decompor cada parte para uma explica√ß√£o mais simples:

### *Comando M√°gico %create_session:*

---

Este √© um comando especial Data Flow Magic usado no Jupyter Notebook para iniciar uma nova sess√£o do Data Flow.

Op√ß√µes do Comando:

- *-l python:* Especifica a linguagem de programa√ß√£o a ser usada, neste caso, Python.
- *-c '{...}':* Cont√©m a configura√ß√£o em formato JSON para a sess√£o do Data Flow.

#### *Configura√ß√µes da Sess√£o:*

---

- *"compartmentId":* Identificador do compartimento na OCI onde a sess√£o ser√° criada.
- *"displayName":* Nome da sess√£o para identifica√ß√£o. Pode ser alterado livremente.
- *"language":* Define a linguagem de programa√ß√£o (Python).
- *"sparkVersion":* Especifica a vers√£o do Spark a ser usada.
- *"numExecutors":* N√∫mero de executores para o Spark. Os executores s√£o processos que executam tarefas e armazenam dados para o aplicativo Spark.
- *"driverShape" e "executorShape":* Especificam o tipo de m√°quina virtual (VM.Standard.E4.Flex) usada para o driver e os executores, respectivamente. O driver √© o processo central que coordena as tarefas do Spark.
- *"driverShapeConfig" e "executorShapeConfig":* Configura√ß√µes detalhadas para o driver e os executores, incluindo o n√∫mero de CPUs (OCPUs) e a quantidade de mem√≥ria (em GBs).
- *"logsBucketUri":* Endere√ßo do bucket (cont√™iner de armazenamento) na OCI onde os logs da sess√£o ser√£o armazenados.
- *"configuration":* Configura√ß√µes adicionais, como a localiza√ß√£o do ambiente do Spark e outras op√ß√µes de configura√ß√£o.
- *"metastoreId":* Um identificador para o metastore de dados usado. Um metastore √© um reposit√≥rio central para armazenar metadados sobre as estruturas de dados, como tabelas e esquemas, dentro do ambiente Spark.

2. Depois de executar a c√©lula que cont√©m o comando m√°gico, aguarde a cria√ß√£o da sess√£o Spark.**

    ![Progresso Cluster](.\images\8-progress-cluster.png)

    A sess√£o do Spark estar√° pronta para ser utilizada assim que a seguinte mensagem for exibida.

    Cada vez que voc√™ cria uma nova sess√£o, um novo "Session ID" √© atribu√≠do a essa sess√£o, permitindo que o ambiente de cluster diferencie entre m√∫ltiplas sess√µes que podem estar ocorrendo simultaneamente. Esse ID pode ser utilizado para retomar, gerenciar ou encerrar a sess√£o espec√≠fica a qualquer momento.

    ![Cluster Pronto](.\images\9-cluster-ready.png)

3. Utilize o comando m√°gico *`%status`* para verificar o status da sess√£o atual.**

    ![Status Cluster](.\images\10-status-cluster.png)

> **Nota:** Os clusters da Sess√£o do OCI Data Flow ficam ativos por 24 horas (1440 minutos) por padr√£o, por√©m voc√™ pode customizar esse per√≠odo para criar sess√µes que permanecer√£o ativas por at√© 7 dias (10,080 minutos)(maxDurationInMinutes).

Parab√©ns, voc√™ terminou esse laborat√≥rio! üéâ

Voc√™ pode **seguir para o pr√≥ximo Lab**.

## Conclus√£o

Nesta laborat√≥rio, voc√™ aprendeu como utilizar notebooks Jupyter no OCI Data Science para criar e gerenciar sess√µes do Data Flow com Apache Spark, abrangendo desde autentica√ß√£o e configura√ß√£o de vari√°veis at√© a execu√ß√£o interativa de c√≥digo e monitoramento do status do cluster.

## Autoria

- **Autores** - Thais Henrique, Heloisa Escobar, Isabelle Anjos
- **√öltimo Update Por/Date** - Isabelle Anjos, Jan/2024