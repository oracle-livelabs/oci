apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  # The failure domain will spread the replicas of the data across different failure zones
  failureDomain: host
  # Ceph CRUSH root location for this pool
  # crushRoot: custom-root
  # The deviceClass is used for placement. If not specified, all devices are used.
  # deviceClass: ssd
  replicated:
    # The number of data copies to make
    size: 3
    # RequireSafeReplicaSize if false allows you to set replica 1
    requireSafeReplicaSize: true
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
# The provisioner name should be the same as the Rook Ceph operator namespace
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
  # clusterID is the namespace where the Rook Ceph cluster is running
  clusterID: rook-ceph
  
  # Ceph pool into which the RBD image shall be created
  pool: replicapool

  # RBD image format. Defaults to "2".
  imageFormat: "2"

  # RBD image features
  imageFeatures: layering

  # The secrets contain Ceph admin credentials.
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

  # Specify the filesystem type of the volume
  csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete

# Allow volume expansion
allowVolumeExpansion: true
