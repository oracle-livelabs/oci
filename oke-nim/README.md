Restructuring the workshop with one intro and 3 labs:

1. Intro 
2. Cluster Configuration 
3. Operator configuration 
4. NIM and Testing Inference 


## Workshop Elevator Pitch/Messaging: Required

In this workshop, you will learn how to deploy and manage NVIDIA NIM containers on Oracle Container Engine for Kubernetes (OKE)  to run scalable, high-performance AI inference workloads. This session is designed for developers and MLOps engineers who want to operationalize NVIDIA AI models using Kubernetes-native workflows on OCI.

## Workshop Description
In this workshop, you will learn how to deploy and manage NVIDIA NIM containers on Oracle Container Engine for Kubernetes (OKE)  to run scalable, high-performance AI inference workloads. This session is designed for developers and MLOps engineers who want to operationalize NVIDIA AI models using Kubernetes-native workflows on OCI.

This workshop is ideal for developers, data scientists, and DevOps practitioners interested in:

- Deploying NIM on Kubernetes: Learn how to orchestrate and scale NVIDIA NIM containers using OKE for efficient AI inference in production.
- Harnessing NVIDIA GPUs for optimal performance: Experience how OKE integrates with NVIDIA GPU nodes to accelerate inference with TensorRT-optimized NIMs.
- Streamlining MLOps: Automate deployment, scaling, and lifecycle management of AI models using Helm, kubectl, and OCI DevOps tools within a unified cloud environment.

What you will learn
By the end of this workshop, you will have hands-on experience with:

- Creating an NVIDIA API Key and pulling NIM containers
- Setting up an OKE cluster with GPU-enabled nodes
- Deploying NVIDIA NIM using Helm or kubectl
- Exposing NIM services securely for inference
- Sending inference requests and monitoring performance with OCI tools

## Why is this workshop needed? Please fill in

## What products/technologies are used? Please fill in

## Is there a primary Oracle product/technology being showcased? If so, what is it? Needed for Workshop Council


## Workshop Abstract

Workshop Elevator Pitch/Messaging: This hands-on lab shows you how to deploy and scale NVIDIA NIM containers on OKE to deliver high-performance AI inference at enterprise scale. You’ll learn how to operationalize NVIDIA AI models with Kubernetes-native workflows, leveraging OCI’s GPU infrastructure for maximum throughput and efficiency. By the end, you’ll have deployed real NIM containers, served live inference requests, and automated the entire lifecycle using Helm, kubectl, and OCI DevOps, turning what’s usually complex AI infrastructure into a repeatable, production-ready workflow.

Workshop Description: In this workshop, you will learn how to deploy and manage NVIDIA NIM containers on OKE to run scalable, high-performance AI inference workloads. This session is designed for developers and MLOps engineers who want to operationalize NVIDIA AI models using Kubernetes-native workflows on OCI.

Why is this workshop needed? Drive Cloud Native/OKE and NVIDIA software/hardware solutions with customers/partners

What products/technologies are used? OKE, NVIDIA GPU, NVIDIA NIM

Is there a primary Oracle product/technology being showcased? If so, what is it? OKE


For OSPA Workshops Only:

What are the training objectives? N/A

What module will this exist in? N/A

## Workshop Outline
Lab 1: Set up OKE Cluster

Lab 2: Deploy the LLM NIM

Lab 3: Testing the inference API

## Workshop Prerequisites

Familiarity with Oracle Cloud Infrastructure (OCI) is helpful

Familiarity with Oracle Kubernetes Engine (OKE) is helpful